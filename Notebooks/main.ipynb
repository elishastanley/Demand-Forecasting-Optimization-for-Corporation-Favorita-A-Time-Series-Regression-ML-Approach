{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Demand Forecasting Optimization for Corporation Favorita A Time Series Regression ML Approach**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "1. [Business Understanding](#business-understanding)\n",
    "2. [Data collection](#Data-collection)\n",
    "3. [Data Cleaning](#Data-Cleaning)\n",
    "4. [Exploratory Data Analysis (EDA)](#exploratory-data-analysis-(EDA))\n",
    "5. [Hypotheses Testing](#Hypotheses-Testing)\n",
    "6. [Analytical Questions](#analytical-questions)\n",
    "\n",
    "\n",
    "## **Business Understanding**\n",
    "\n",
    "Corporation Favorita, a large grocery retailer based in Ecuador, aims to optimize its inventory management to ensure the right quantity of products is always in stock across its various locations. Effective inventory management is critical for maintaining high levels of customer satisfaction and minimizing costs associated with overstocking or stockouts.\n",
    "\n",
    "To achieve this goal, we will build machine learning models to forecast the demand for products at different Favorita stores. Accurate demand forecasting will allow Corporation Favorita to make informed decisions regarding stock levels, promotions, and supply chain logistics.\n",
    "\n",
    "#### Objective:\n",
    "1. **Develop Predictive Models**: Create models to forecast daily unit sales for thousands of products across multiple stores.\n",
    "2. **Understand Influencing Factors**: Analyze the impact of various factors on sales, including promotions, holidays, oil prices, store characteristics, and external events such as earthquakes.\n",
    "3. **Optimize Inventory Management**: Use the models to inform inventory decisions, ensuring that the right products are available at the right stores at the right time.\n",
    "\n",
    "#### Hypothesis:\n",
    "1. Null Hypothesis (H0): Promotions do not have a significant impact on sales.  \n",
    "   Alternative Hypothesis (H1): Promotions have a significant impact on sales.\n",
    "2. Null Hypothesis (H0): Oil prices do not significantly impact sales.  \n",
    "   Alternative Hypothesis (H1): Oil prices significantly impact sales.\n",
    "\n",
    "#### Key Analytical Questions:\n",
    "1. **Data Completeness**: Is the training dataset complete with all required dates?\n",
    "2. **Sales Extremes**: Which dates have the lowest and highest sales for each year (excluding days when stores were closed)?\n",
    "3. **Monthly Sales Trends**: Compare sales across months and years to identify the highest sales month and year.\n",
    "4. **Earthquake Impact**: Did the earthquake in April 2016 impact sales?\n",
    "5. **Store Performance**: Are certain stores or groups of stores (cluster, city, state, type) selling more products?\n",
    "6. **Promotions, Oil Prices, and Holidays**: How are sales affected by promotions, oil prices, and holidays?\n",
    "7. **Date Features Analysis**: What insights can be derived from date-related features?\n",
    "8. **Promotion Impact**: Which product families and stores were most affected by promotions?\n",
    "9. **Error Metrics**: What is the difference between RMSLE, RMSE, and MSE? Why might MAE be greater than these metrics?\n",
    "10. **Wage Payment Influence**: Does the bi-monthly payment of public sector wages influence store sales?\n",
    "\n",
    "\n",
    "#### Data Sources\n",
    "- **train.csv**: Time series data of store, product information, promotions, and sales.\n",
    "- **test.csv**: Features similar to the training data for the 15 days following the last date in the training data.\n",
    "- **transaction.csv**: Daily transactions per store.\n",
    "- **stores.csv**: Metadata about stores including location and type.\n",
    "- **oil.csv**: Daily oil prices.\n",
    "- **holidays_events.csv**: Information about holidays and events, including special designations like transferred, bridge, and work days.\n",
    "- **sample_submission.csv**: A sample submission file for formatting predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Importing Necessary packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "from dotenv import dotenv_values\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import product\n",
    "from scipy import stats\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from scipy.stats import ttest_ind\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.ticker as ticker\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.calibration import LabelEncoder\n",
    "from pmdarima.arima import auto_arima\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Seaborn color palette\n",
    "palette = sns.color_palette(\"tab10\")\n",
    "\n",
    "# Function to format y-axis labels in millions\n",
    "def millions(x, pos):\n",
    "    return '%1.0fM' % (x * 1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Collection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading environment variables from .env file\n",
    "environment_variables = dotenv_values('.env')\n",
    "\n",
    "# Getting the values for the credentials set in the .env file\n",
    "server = environment_variables.get(\"SERVER\")\n",
    "database = environment_variables.get(\"DATABASE\")\n",
    "username = environment_variables.get(\"USERNAME\")\n",
    "password = environment_variables.get(\"PASSWORD\")\n",
    "\n",
    "# Creating a connection string\n",
    "connection_string = f\"DRIVER={{SQL Server}}; \\\n",
    "                    SERVER={server}; \\\n",
    "                    DATABASE={database}; \\\n",
    "                    UID={username}; \\\n",
    "                    PWD={password};\"\n",
    "\n",
    "# Connecting to the server\n",
    "connection = pyodbc.connect(connection_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data from the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Oil dataset\n",
    "oil = pd.read_sql_query(\"SELECT * FROM dbo.oil\", connection)\n",
    "\n",
    "# Saving the DataFrame to a CSV file\n",
    "oil.to_csv('data/oil.csv', index=False)\n",
    "\n",
    "oil.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading holidays_events dataset\n",
    "holidays_events = pd.read_sql_query(\n",
    "    \"SELECT * FROM dbo.holidays_events\", connection)\n",
    "\n",
    "# Saving the DataFrame to a CSV file\n",
    "holidays_events.to_csv('data/holidays_events.csv', index=False)\n",
    "\n",
    "holidays_events.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading stores dataset\n",
    "stores = pd.read_sql_query(\"SELECT * FROM dbo.stores\", connection)\n",
    "\n",
    "# Saving the DataFrame to a CSV file\n",
    "stores.to_csv('data/stores.csv', index=False)\n",
    "\n",
    "stores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('Data/train.csv')\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('Data/test.csv')\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = pd.read_csv('Data/transactions.csv')\n",
    "\n",
    "transactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('Data/sample_submission.csv')\n",
    "\n",
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Missing Values Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values in each dataset\n",
    "missing_values_train = train.isnull().sum()\n",
    "missing_values_test = test.isnull().sum()\n",
    "missing_values_transactions = transactions.isnull().sum()\n",
    "missing_values_stores = stores.isnull().sum()\n",
    "missing_values_oil = oil.isnull().sum()\n",
    "missing_values_holidays_events = holidays_events.isnull().sum()\n",
    "missing_values_sample_submission = sample_submission.isnull().sum()\n",
    "\n",
    "missing_values_train, missing_values_test, missing_values_transactions, missing_values_stores, missing_values_oil, missing_values_holidays_events, missing_values_sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling missing values in the oil dataset using forward fill\n",
    "oil['dcoilwtico'].fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Verifying that there are no more missing values\n",
    "missing_values_oil = oil.isnull().sum()\n",
    "missing_values_oil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling the remaining missing value using backward fill\n",
    "oil['dcoilwtico'].fillna(method='bfill', inplace=True)\n",
    "\n",
    "# Verifying that there are no more missing values\n",
    "missing_values_oil = oil.isnull().sum()\n",
    "missing_values_oil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Data Types Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting date columns to datetime format\n",
    "train['date'] = pd.to_datetime(train['date'])\n",
    "test['date'] = pd.to_datetime(test['date'])\n",
    "transactions['date'] = pd.to_datetime(transactions['date'])\n",
    "oil['date'] = pd.to_datetime(oil['date'])\n",
    "holidays_events['date'] = pd.to_datetime(holidays_events['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Merging Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging train dataset with transactions, stores, oil, and holidays_events\n",
    "train_merged = train.merge(transactions, on=['date', 'store_nbr'], how='left')\n",
    "train_merged = train_merged.merge(stores, on='store_nbr', how='left')\n",
    "train_merged = train_merged.merge(oil, on='date', how='left')\n",
    "train_merged = train_merged.merge(holidays_events, on='date', how='left')\n",
    "\n",
    "# Repeating the same for the test dataset\n",
    "test_merged = test.merge(transactions, on=['date', 'store_nbr'], how='left')\n",
    "test_merged = test_merged.merge(stores, on='store_nbr', how='left')\n",
    "test_merged = test_merged.merge(oil, on='date', how='left')\n",
    "test_merged = test_merged.merge(holidays_events, on='date', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary renames \n",
    "train_merged.rename(columns={'type_x': 'store_type','type_y': 'holiday_type'}, inplace=True)\n",
    "test_merged.rename(columns={'type_x': 'store_type','type_y': 'holiday_type'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling null values in transactions with 0\n",
    "train_merged['transactions'].fillna(0, inplace=True)\n",
    "test_merged['transactions'].fillna(0, inplace=True)\n",
    "\n",
    "# Forward filling and backward filling for dcoilwtico\n",
    "train_merged['dcoilwtico'].fillna(method='ffill', inplace=True)\n",
    "train_merged['dcoilwtico'].fillna(method='bfill', inplace=True)\n",
    "\n",
    "test_merged['dcoilwtico'].fillna(method='ffill', inplace=True)\n",
    "test_merged['dcoilwtico'].fillna(method='bfill', inplace=True)\n",
    "\n",
    "# Filling holiday-related columns with 'None'\n",
    "holiday_related_cols = ['holiday_type', 'locale','locale_name', 'description', 'transferred']\n",
    "train_merged[holiday_related_cols] = train_merged[holiday_related_cols].fillna('None')\n",
    "test_merged[holiday_related_cols] = test_merged[holiday_related_cols].fillna('None')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Creating Date Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create additional features from the date\n",
    "train_merged['year'] = train_merged['date'].dt.year\n",
    "train_merged['month'] = train_merged['date'].dt.month\n",
    "train_merged['day'] = train_merged['date'].dt.day\n",
    "train_merged['dayofweek'] = train_merged['date'].dt.dayofweek\n",
    "\n",
    "test_merged['year'] = test_merged['date'].dt.year\n",
    "test_merged['month'] = test_merged['date'].dt.month\n",
    "test_merged['day'] = test_merged['date'].dt.day\n",
    "test_merged['dayofweek'] = test_merged['date'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exploratory Data Analysis (EDA)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Grouping by date and sum sales to get the overall sales trend over time\n",
    "# sales_trend = train_merged.groupby('date')['sales'].sum().reset_index()\n",
    "\n",
    "# # Grouping by year and month for monthly trend\n",
    "# sales_trend['year_month'] = sales_trend['date'].dt.to_period('M')\n",
    "# monthly_sales_trend = sales_trend.groupby(\n",
    "#     'year_month')['sales'].sum().reset_index()\n",
    "\n",
    "# # Grouping by year for yearly trend\n",
    "# sales_trend['year'] = sales_trend['date'].dt.year\n",
    "# yearly_sales_trend = sales_trend.groupby('year')['sales'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plotting overall sales trend\n",
    "# plt.figure(figsize=(14, 7))\n",
    "# plt.plot(sales_trend['date'], sales_trend['sales'],\n",
    "#          color=palette[0], linewidth=1, label='Daily Sales')\n",
    "# plt.title('Overall Sales Trend Over Time', fontsize=16)\n",
    "# plt.xlabel('Date', fontsize=14)\n",
    "# plt.ylabel('Total Sales', fontsize=14)\n",
    "# plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "# # Formatting y-axis to avoid exponents\n",
    "# plt.gca().get_yaxis().get_major_formatter().set_scientific(False)\n",
    "\n",
    "# # Adding a trend line for better visualization\n",
    "# z = np.polyfit(sales_trend['date'].apply(lambda x: x.toordinal()), sales_trend['sales'], 1)\n",
    "# p = np.poly1d(z)\n",
    "# plt.plot(sales_trend['date'], p(sales_trend['date'].apply(lambda x: x.toordinal())), \n",
    "#          color=palette[1], linestyle='--', linewidth=2, label='Trend Line')\n",
    "\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plotting yearly sales trend\n",
    "# plt.figure(figsize=(14, 7))\n",
    "# plt.plot(yearly_sales_trend['year'], yearly_sales_trend['sales'],\n",
    "#          color=palette[0], linewidth=2, label='Yearly Sales')\n",
    "# plt.title('Yearly Sales Trend', fontsize=16)\n",
    "# plt.xlabel('Year', fontsize=14)\n",
    "# plt.ylabel('Total Sales', fontsize=14)\n",
    "# plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "# plt.gca().get_yaxis().set_major_formatter(FuncFormatter(millions))\n",
    "\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sales Distribution Across Stores and Product Families"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Aggregate sales by store and product family\n",
    "# store_sales = train_merged.groupby('store_nbr')['sales'].sum(\n",
    "# ).sort_values(ascending=False).reset_index()\n",
    "# family_sales = train_merged.groupby(\n",
    "#     'family')['sales'].sum().sort_values(ascending=False).reset_index()\n",
    "\n",
    "# # Plot sales distribution across stores\n",
    "# plt.figure(figsize=(14, 7))\n",
    "# plt.bar(store_sales['store_nbr'], store_sales['sales'])\n",
    "# plt.xlabel('Store Number')\n",
    "# plt.ylabel('Total Sales')\n",
    "# plt.title('Sales Distribution Across Stores')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot sales distribution across product families\n",
    "# plt.figure(figsize=(14, 7))\n",
    "# plt.barh(family_sales['family'], family_sales['sales'])\n",
    "# plt.xlabel('Total Sales')\n",
    "# plt.ylabel('Product Family')\n",
    "# plt.title('Sales Distribution Across Product Families')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impact of Promotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Aggregate sales by promotion status\n",
    "# promotion_sales = train_merged.groupby(\n",
    "#     'onpromotion')['sales'].sum().reset_index()\n",
    "\n",
    "# # Plot impact of promotions on sales\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.bar(promotion_sales['onpromotion'],\n",
    "#         promotion_sales['sales'], color=['blue', 'orange'])\n",
    "# plt.xlabel('On Promotion')\n",
    "# plt.ylabel('Total Sales')\n",
    "# plt.title('Impact of Promotions on Sales')\n",
    "# plt.xticks([0, 1], ['No', 'Yes'])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impact of Oil Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot sales versus oil prices\n",
    "# plt.figure(figsize=(14, 7))\n",
    "# plt.plot(train_merged['date'], train_merged['sales'], label='Sales')\n",
    "# plt.plot(train_merged['date'], train_merged['dcoilwtico'],\n",
    "#          label='Oil Price', color='orange')\n",
    "# plt.xlabel('Date')\n",
    "# plt.ylabel('Value')\n",
    "# plt.title('Sales vs Oil Prices')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impact of Holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Aggregate sales by holiday status\n",
    "# holiday_sales = train_merged.groupby(\n",
    "#     'holiday_type')['sales'].sum().reset_index()\n",
    "\n",
    "# # Plot impact of holidays on sales\n",
    "# plt.figure(figsize=(14, 7))\n",
    "# plt.bar(holiday_sales['holiday_type'], holiday_sales['sales'], color='green')\n",
    "# plt.xlabel('Holiday Type')\n",
    "# plt.ylabel('Total Sales')\n",
    "# plt.title('Impact of Holidays on Sales')\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Hypotheses Testing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null Hypothesis (H0a): Promotions do not have a significant impact on sales.  \n",
    "\n",
    "Alternative Hypothesis (H1a): Promotions have a significant impact on sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into promotional and non-promotional periods\n",
    "promotional_sales = train[train['onpromotion'] > 0]['sales']\n",
    "non_promotional_sales = train[train['onpromotion'] == 0]['sales']\n",
    "\n",
    "# Calculating descriptive statistics\n",
    "promotional_mean = promotional_sales.mean()\n",
    "non_promotional_mean = non_promotional_sales.mean()\n",
    "promotional_std = promotional_sales.std()\n",
    "non_promotional_std = non_promotional_sales.std()\n",
    "\n",
    "print(f\"Promotional Sales - Mean: {promotional_mean}, Std: {promotional_std}\")\n",
    "print(f\"Non-Promotional Sales - Mean: {non_promotional_mean}, Std: {non_promotional_std}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing t-test\n",
    "t_stat, p_value = ttest_ind(promotional_sales, non_promotional_sales)\n",
    "\n",
    "print(f\"T-Statistic: {t_stat}, P-Value: {p_value}\")\n",
    "\n",
    "# Interpreting the results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"We Reject the null hypothesis (H0a). Promotions have a significant impact on sales.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis (H0a). Promotions do not have a significant impact on sales.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null Hypothesis (H0b): Oil prices do not significantly impact sales.  \n",
    "\n",
    "Alternative Hypothesis (H1b): Oil prices significantly impact sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing rows with missing oil prices\n",
    "oil_merged_data = train_merged.dropna(subset=['dcoilwtico'])\n",
    "\n",
    "# Performing correlation analysis\n",
    "correlation = oil_merged_data['sales'].corr(oil_merged_data['dcoilwtico'])\n",
    "print(f\"Correlation between sales and oil prices: {correlation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing linear regression analysis\n",
    "X = oil_merged_data['dcoilwtico']\n",
    "y = oil_merged_data['sales']\n",
    "X = sm.add_constant(X)  # Adds a constant term to the predictor\n",
    "\n",
    "model = sm.OLS(y, X).fit()\n",
    "summary = model.summary()\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the p-value for the oil price coefficient\n",
    "p_value = model.pvalues['dcoilwtico']\n",
    "print(f\"P-Value for oil prices: {p_value}\")\n",
    "\n",
    "# Interpret the results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis (H0b). Oil prices significantly impact sales.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis (H0b). Oil prices do not significantly impact sales.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Analytical Questions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Checking if the train dataset complete (has all the required dates)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining the start and end dates in the train dataset\n",
    "start_date = train['date'].min()\n",
    "end_date = train['date'].max()\n",
    "\n",
    "# Creating a complete date range from start to end date\n",
    "complete_date_range = pd.date_range(start=start_date, end=end_date)\n",
    "\n",
    "# Extracting the unique dates from the train dataset\n",
    "unique_dates_in_train = train['date'].unique()\n",
    "\n",
    "# Finding any missing dates by comparing the complete date range to the dates in the train dataset\n",
    "missing_dates = set(complete_date_range) - set(unique_dates_in_train)\n",
    "\n",
    "print(f\"Start Date in Train Dataset: {start_date}\")\n",
    "print(f\"End Date in Train Dataset: {end_date}\")\n",
    "print(f\"Number of Unique Dates in Train Dataset: {len(unique_dates_in_train)}\")\n",
    "print(f\"Number of Dates in Complete Range: {len(complete_date_range)}\")\n",
    "print(f\"Missing Dates: {sorted(missing_dates)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Determining which dates have the lowest and highest sales for each year(excluding days the store was closed)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying dates when stores were closed (i.e., zero transactions)\n",
    "closed_dates = transactions[transactions['transactions'] == 0]['date'].unique()\n",
    "\n",
    "# Filtering out closed dates from the sales data\n",
    "sales_data = train_merged[~train_merged['date'].isin(closed_dates)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing dictionaries to store the results\n",
    "lowest_sales_dates = {}\n",
    "highest_sales_dates = {}\n",
    "\n",
    "# Grouping by year and find the dates with the lowest and highest sales\n",
    "for year, group in sales_data.groupby('year'):\n",
    "    lowest_sales_row = group.loc[group['sales'].idxmin()]\n",
    "    highest_sales_row = group.loc[group['sales'].idxmax()]\n",
    "    lowest_sales_dates[year] = (lowest_sales_row['date'], lowest_sales_row['sales'])\n",
    "    highest_sales_dates[year] = (highest_sales_row['date'], highest_sales_row['sales'])\n",
    "\n",
    "# Creating DataFrames to display the results\n",
    "lowest_sales_df = pd.DataFrame(list(lowest_sales_dates.items()), \n",
    "                               columns=['Year', 'Lowest Sales Data'])\n",
    "highest_sales_df = pd.DataFrame(list(highest_sales_dates.items()), \n",
    "                                columns=['Year', 'Highest Sales Data'])\n",
    "\n",
    "# Splitting the tuple into separate columns\n",
    "lowest_sales_df[['Lowest Sales Date', 'Lowest Sales']] = pd.DataFrame(\n",
    "    lowest_sales_df['Lowest Sales Data'].tolist(), index=lowest_sales_df.index)\n",
    "highest_sales_df[['Highest Sales Date', 'Highest Sales']] = pd.DataFrame(\n",
    "    highest_sales_df['Highest Sales Data'].tolist(), index=highest_sales_df.index)\n",
    "\n",
    "# Dropping the original tuple columns\n",
    "lowest_sales_df.drop(columns=['Lowest Sales Data'], inplace=True)\n",
    "highest_sales_df.drop(columns=['Highest Sales Data'], inplace=True)\n",
    "\n",
    "print(\"Dates with the Lowest Sales for Each Year:\")\n",
    "print(lowest_sales_df)\n",
    "print(\"\\nDates with the Highest Sales for Each Year:\")\n",
    "print(highest_sales_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Comparing the sales for each month across the years and determine which month of which year had the highest sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by date and sum sales to get the overall sales trend over time\n",
    "sales_trend = train_merged.groupby('date')['sales'].sum().reset_index()\n",
    "\n",
    "# Grouping by year and month for monthly trend\n",
    "sales_trend['year_month'] = sales_trend['date'].dt.to_period('M')\n",
    "monthly_sales_trend = sales_trend.groupby('year_month')['sales'].sum().reset_index()\n",
    "\n",
    "\n",
    "# Plotting monthly sales trend\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(monthly_sales_trend['year_month'].astype(str),\n",
    "         monthly_sales_trend['sales'], color=palette[0], linewidth=2, label='Monthly Sales')\n",
    "plt.title('Monthly Sales Trend', fontsize=18, weight='bold')\n",
    "plt.xlabel('Year-Month', fontsize=14)\n",
    "plt.ylabel('Total Sales', fontsize=14)\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Formatting y-axis exponents\n",
    "plt.gca().get_yaxis().set_major_formatter(FuncFormatter(millions))\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **General Trend**:\n",
    "   - The overall trend indicates a steady increase in monthly sales from 2013 to 2017.\n",
    "   - There are noticeable peaks and troughs, indicating seasonal variations and possibly promotional events or holidays affecting sales.\n",
    "\n",
    "2. **Highest Sales Month**:\n",
    "   - The highest sales month across all years is `May 2016`, with sales reaching approximately `30 million`. This peak is significantly higher than other months, suggesting a major event or promotion during that period.\n",
    "\n",
    "3. **Seasonal Patterns**:\n",
    "   - Sales tend to peak towards the end of the year, particularly in November and December, likely due to holiday shopping.\n",
    "   - Another notable peak occurs around mid-year (June and July), which may correspond to mid-year sales events or summer holidays.\n",
    "\n",
    "4. **Year-over-Year Comparison**:\n",
    "   - Each year shows an increasing trend in sales for most months, indicating growth in business and possibly an expanding customer base.\n",
    "   - The months of November and December consistently show higher sales across the years, reinforcing the impact of holiday shopping on sales performance.\n",
    "\n",
    "5. **Significant Peaks**:\n",
    "   - The graph shows significant peaks not only in May 2016 but also in December 2014, and November 2013, indicating these months had exceptionally high sales compared to their respective years.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Did the earthquake impact sales?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:A magnitude 7.8 earthquake struck Ecuador on April 16, 2016. People rallied in relief efforts donating water and other first need products which greatly affected supermarket sales for several weeks after the earthquake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the period around the earthquake date (1 month before and 1 month after)\n",
    "start_date = '2016-03-16'\n",
    "end_date = '2016-05-16'\n",
    "earthquake_date = '2016-04-16'\n",
    "\n",
    "# Filtering data for the defined period\n",
    "filtered_data = train[(train['date'] >= start_date) & (train['date'] <= end_date)]\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(filtered_data['date'], filtered_data['sales'], marker='o', linestyle='-', color=palette[0], label='Daily Sales')\n",
    "plt.axvline(pd.to_datetime(earthquake_date), color=palette[1], linestyle='--', label='Earthquake Date')\n",
    "plt.title('Sales Trend Around Earthquake Date (April 16, 2016)',\n",
    "          fontsize=18, weight='bold')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Immediate Sales Increase Post-Earthquake**:\n",
    "   - There is a noticeable spike in sales immediately after the earthquake on April 16, 2016. This spike is evident within the first few days following the earthquake and is significantly higher compared to the sales before the earthquake.\n",
    "\n",
    "2. **Sustained High Sales**:\n",
    "   - Sales remain elevated for several weeks following the earthquake. This indicates that the earthquake had a prolonged impact on sales, likely due to ongoing relief efforts and increased demand for essential goods.\n",
    "\n",
    "3. **Peak Sales**:\n",
    "   - The highest sales point within the observed period occurs shortly after the earthquake, suggesting a direct correlation between the event and the surge in sales.\n",
    "\n",
    "4. **Pre-Earthquake Sales Stability**:\n",
    "   - Sales prior to the earthquake show a relatively stable pattern with minor fluctuations, indicating a normal sales period before the disruptive event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining periods before and after the earthquake\n",
    "before_earthquake = filtered_data[filtered_data['date'] < earthquake_date]\n",
    "after_earthquake = filtered_data[filtered_data['date'] >= earthquake_date]\n",
    "\n",
    "# Performing t-test\n",
    "t_stat, p_value = ttest_ind(\n",
    "    before_earthquake['sales'], after_earthquake['sales'])\n",
    "\n",
    "print(f\"T-Statistic: {t_stat}, P-Value: {p_value}\")\n",
    "\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"We Reject the null hypothesis. The earthquake had a significant impact on sales.\")\n",
    "else:\n",
    "    print(\"We Fail to reject the null hypothesis. The earthquake did not have a significant impact on sales.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Are certain groups of stores selling more products? (Cluster, city, state, type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sales by Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by cluster and calculate total sales\n",
    "cluster_sales = train_merged.groupby('cluster')['sales'].sum().reset_index()\n",
    "cluster_sales = cluster_sales.sort_values(by='cluster', ascending= True)\n",
    "\n",
    "# Plotting total sales by cluster\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(x='cluster', y='sales', data=cluster_sales, palette=palette)\n",
    "plt.title('Total Sales by Cluster', fontsize=20, weight='bold')\n",
    "plt.xlabel('Cluster', fontsize=16)\n",
    "plt.ylabel('Total Sales', fontsize=16)\n",
    "plt.gca().get_yaxis().set_major_formatter(plt.FuncFormatter(millions))\n",
    "plt.xticks(ticks=range(len(cluster_sales)),\n",
    "           labels=cluster_sales['cluster'].astype(int), fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Cluster with Highest Sales**:\n",
    "  - Cluster `14` stands out with the highest total sales, significantly outperforming other clusters with over `160M` in sales. This indicates a particularly strong performance, possibly due to a higher number of stores, larger store sizes, or a more affluent customer base.\n",
    "\n",
    "- **Other High-Performing Clusters**:\n",
    "  - Clusters `6` and `8` also show high sales, with each generating over `120M` in sales. These clusters, along with cluster 14, make up the top three clusters in terms of sales performance.\n",
    "  - Clusters `3`, `11`, and `13` follow closely behind, each with sales ranging between `80M` and `100M`. These clusters represent the mid-high range of sales performance.\n",
    "\n",
    "- **Lower-Performing Clusters**:\n",
    "  - Clusters `2`, `7`, `9`, `12`, and `16` have the lowest total sales, each generating less than `40M`. These clusters might consist of fewer stores, smaller store sizes, or be located in regions with lower customer demand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sales by City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by city and calculate total sales\n",
    "city_sales = train_merged.groupby('city')['sales'].sum().reset_index()\n",
    "city_sales = city_sales.sort_values(by='sales', ascending=False)\n",
    "\n",
    "# Plotting total sales by city\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(x='sales', y='city', data=city_sales, palette=palette, orient='h')\n",
    "plt.title('Total Sales by City ', fontsize=20, weight='bold')\n",
    "plt.xlabel('Total Sales', fontsize=16)\n",
    "plt.ylabel('City', fontsize=16)\n",
    "plt.gca().get_xaxis().set_major_formatter(plt.FuncFormatter(millions))\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **City with Highest Sales**:\n",
    "  - `Quito` stands out with the highest total sales, significantly outperforming other cities with over `500M` in sales. This indicates that Quito is a major commercial hub with high customer demand and purchasing power.\n",
    "\n",
    "- **Other High-Performing Cities**:\n",
    "  - `Guayaquil` comes second at `120M`sale sales, `Cuenca` follows with around `80M` in sales, showing strong performance as well. It is a key city with substantial sales figures.\n",
    "  - `Ambato` is next with over `40M` in sales, indicating a solid market presence.\n",
    "  - Other notable cities include `Cayambe` and `Daule`, each contributing significantly to the total sales but with lower figures compared to the top three cities.\n",
    "\n",
    "- **Lower-Performing Cities**:\n",
    "  - Cities like `Babahoyo`, `El Carmen`, and `Guaranda` show relatively lower total sales, each generating less than `20M`. These cities might have fewer stores, smaller store sizes, or be located in regions with lower customer demand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sales by state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by state and calculate total sales\n",
    "state_sales = train_merged.groupby('state')['sales'].sum().reset_index()\n",
    "state_sales = state_sales.sort_values(by='sales', ascending=False)\n",
    "\n",
    "# Plotting total sales by state\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(x='sales', y='state', data=state_sales,\n",
    "            palette=palette, orient='h')\n",
    "plt.title('Total Sales by State', fontsize=20, weight='bold')\n",
    "plt.xlabel('Total Sales', fontsize=16)\n",
    "plt.ylabel('State', fontsize=16)\n",
    "plt.gca().get_xaxis().set_major_formatter(plt.FuncFormatter(millions))\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **State with Highest Sales**:\n",
    "  - `Pichincha` dominates with the highest total sales, significantly outperforming other states with `600M` in sales. This indicates that Pichincha, likely due to the presence of the capital city `Quito`, is a major commercial hub with high customer demand.\n",
    "\n",
    "- **Other High-Performing States**:\n",
    "  - `Guayas` follows with just below `200M` in sales, showing strong performance as well. It is a key state with substantial sales figures.\n",
    "  - `Azuay`, `Santo Domingo de los TsÃ¡chilas`, and other states each contribute significantly to the total sales but with lower figures compared to the top two states.\n",
    "\n",
    "- **Lower-Performing States**:\n",
    "  - States like `Pastaza`, `Chimborazo`, and `Santa Elena` show relatively lower total sales, each generating less than 10M. These states might have fewer stores, smaller store sizes, or be located in regions with lower customer demand.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sales by Store Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by type and calculate total sales\n",
    "type_sales = train_merged.groupby('store_type')['sales'].sum().reset_index()\n",
    "type_sales = type_sales.sort_values(by='sales', ascending=False)\n",
    "\n",
    "# Plotting total sales by type\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(x='store_type', y='sales', data=type_sales.sort_values(\n",
    "    by='sales', ascending=False), palette=palette)\n",
    "plt.title('Total Sales by Type', fontsize=20, weight='bold')\n",
    "plt.xlabel('Type', fontsize=16)\n",
    "plt.ylabel('Total Sales', fontsize=16)\n",
    "plt.gca().get_yaxis().set_major_formatter(plt.FuncFormatter(millions))\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Type with Highest Sales**:\n",
    "  - Store types `A` and `D` dominate with the highest total sales, each generating over `350M`. This indicates that these store types are the most popular or have the highest number of stores.\n",
    "\n",
    "- **Other High-Performing Types**:\n",
    "  - Store types `C` and `B` follow with substantial sales, over `150M` each. These types also contribute significantly to the overall sales.\n",
    "\n",
    "- **Lower-Performing Types**:\n",
    "  - Store type `E` shows the lowest total sales, significantly less than the other types, with sales just over `50M`. This type might represent fewer stores or stores with lower customer demand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Are sales affected by promotions, oil prices and holidays?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impact of Promotions on Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping the train data by date and sum the sales and onpromotion columns\n",
    "promotions_sales_df = train_merged.groupby('date').agg(\n",
    "    {'sales': 'sum', 'onpromotion': 'sum'}).reset_index()\n",
    "\n",
    "# Plotting sales vs. promotions\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(x='onpromotion', y='sales', data=promotions_sales_df, palette=palette)\n",
    "plt.title('Sales vs. Promotions', fontsize=18, weight='bold')\n",
    "plt.xlabel('Number of Promotions')\n",
    "plt.ylabel('Sales')\n",
    "ax = plt.gca()\n",
    "ax.yaxis.set_major_formatter(ticker.FuncFormatter(millions))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the correlation between sales and onpromotion\n",
    "promotions_correlation = promotions_sales_df['sales'].corr(promotions_sales_df['onpromotion'])\n",
    "promotions_correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Positive Correlation:**\n",
    "   - The correlation coefficient is approximately `0.60`, indicating a moderate positive relationship between the number of promotions and sales. As the number of promotions increases, sales generally tend to increase.\n",
    "\n",
    "2. **Outliers:**\n",
    "   - There are notable outliers, particularly `one` data point where the number of promotions reaches around `40,000` and sales exceed `3` million. These outliers can significantly influence the overall analysis and warrant further investigation.\n",
    "\n",
    "3. **Diminishing Returns:**\n",
    "   - Beyond approximately `10,000` promotions, the increase in sales does not proportionally match the increase in promotions. This indicates diminishing returns on promotions, where additional promotions yield smaller increases in sales.\n",
    "\n",
    "4. **Cluster of Data Points:**\n",
    "   - A significant cluster of data points is observed between `0` to `10,000` promotions, with sales ranging mostly between `0` to `1` million. This indicates that most promotional activities fall within this range and still generate substantial sales.\n",
    "\n",
    "5. **Non-linear Relationship:**\n",
    "   - While the overall trend shows a positive correlation, the relationship between promotions and sales is not strictly linear. For example, increasing promotions from `20,000` to `30,000` does not lead to a proportionate increase in sales, suggesting other factors may also be influencing sales performance.\n",
    "\n",
    "6. **Marketing Strategy Implications:**\n",
    "   - The analysis suggests that businesses should evaluate the optimal number of promotions needed to maximize sales. Excessive promotional activities, particularly beyond `10,000` promotions, appear to yield diminishing returns and may not be cost-effective.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impact of Oil Prices on Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping the merged data by date and sum the sales\n",
    "sales_oil_df_grouped = train_merged.groupby('date').agg({'sales': 'sum', 'dcoilwtico': 'mean'}).reset_index()\n",
    "\n",
    "# Plotting sales vs. oil prices\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(x='dcoilwtico', y='sales', data=sales_oil_df_grouped, palette=palette)\n",
    "plt.title('Sales vs. Oil Prices', fontsize=16, weight='bold')\n",
    "plt.xlabel('Oil Prices')\n",
    "plt.ylabel('Sales')\n",
    "ax = plt.gca()\n",
    "ax.yaxis.set_major_formatter(ticker.FuncFormatter(millions))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the correlation between sales and oil prices\n",
    "oil_correlation = sales_oil_df_grouped['sales'].corr(sales_oil_df_grouped['dcoilwtico'])\n",
    "oil_correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Negative Correlation:**\n",
    "   - The correlation coefficient is approximately `-0.55`, indicating a moderate negative relationship between oil prices and sales. This suggests that as oil prices increase, sales tend to decrease.\n",
    "\n",
    "2. **Clusters of Data Points:**\n",
    "   - Three main clusters of data points are visible:\n",
    "     - **Oil Prices between `40` and `60`:** This cluster shows a wide range of sales values, mostly between `0` and `2` million.\n",
    "     - **Oil Prices around `80`:** This cluster has fewer data points, with sales generally below `1` million.\n",
    "     - **Oil Prices between `100` and `110`:** This cluster also shows a range of sales values, mostly between `0` and `1` million.\n",
    "\n",
    "3. **Outliers:**\n",
    "   - There are a few outliers where sales exceed `3` million, regardless of oil prices. These points can significantly influence the overall analysis and may require further investigation to understand the context behind such high sales.\n",
    "\n",
    "4. **Distribution of Sales:**\n",
    "   - The majority of sales data points are below `1` million, regardless of oil price levels. However, there is a notable density of sales below `2` million when oil prices are between `40` and `60`.\n",
    "\n",
    "5. **Potential Influences:**\n",
    "   - The moderate negative correlation suggests that higher oil prices may have an adverse effect on sales, possibly due to increased costs for consumers and businesses, reducing their purchasing power or expenditure on other goods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impact of Holidays on Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic summary statistics of sales on holidays vs. non-holidays\n",
    "sales_summary = train_merged.groupby('holiday_type')['sales'].describe()\n",
    "\n",
    "sales_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Additional Holidays**:\n",
    "   - Highest average sales among all holiday types with an average of `487.63`.\n",
    "   - Standard deviation is high at `1414.27`, indicating a wide range of sales values.\n",
    "   - Median sales (50th percentile) is `18`, suggesting that a majority of sales values are clustered at the lower end despite the high mean.\n",
    "\n",
    "2. **Bridge Holidays**:\n",
    "   - Second highest average sales at `446.75`.\n",
    "   - Standard deviation of `1309.72`, showing a significant spread in the data.\n",
    "   - Median sales are `16`, similar to Additional Holidays, showing a lower concentration of sales.\n",
    "\n",
    "3. **Events**:\n",
    "   - Average sales are `425.66`, slightly lower than Bridge Holidays.\n",
    "   - High standard deviation of `1401.02`.\n",
    "   - Median sales of `16`, indicating a concentration of sales towards the lower end.\n",
    "\n",
    "4. **Holidays**:\n",
    "   - Average sales drop to `358.43`.\n",
    "   - Standard deviation is lower at `1153.16` compared to Additional, Bridge, and Event holidays.\n",
    "   - Median sales are `10`, showing a significant portion of sales are lower.\n",
    "\n",
    "5. **Non-Holidays (None)**:\n",
    "   - Average sales are `352.16`, the lowest among all categories except for Work Day.\n",
    "   - Standard deviation of `1076.08`.\n",
    "   - Median sales are `11`, indicating a higher concentration of lower sales values.\n",
    "\n",
    "6. **Transfer Holidays**:\n",
    "   - Average sales are relatively high at `467.75`.\n",
    "   - Standard deviation is `1373.06`.\n",
    "   - Median sales are `24`, higher than the median sales of other holiday types, indicating relatively higher sales even at the lower end.\n",
    "\n",
    "7. **Work Days**:\n",
    "   - Average sales are `372.16`.\n",
    "   - Standard deviation of `1167.57`.\n",
    "   - Median sales are `8`, showing a lower concentration of sales compared to other holiday types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by holiday type to get the total sales for each type\n",
    "sales_by_holiday_type = train_merged.groupby('holiday_type')['sales'].mean().reset_index()\n",
    "\n",
    "# Plotting average sales for different holiday types\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='holiday_type', y='sales',data=sales_by_holiday_type, palette=palette)\n",
    "plt.title('Average Sales for Different Holiday Types',fontsize=16, weight='bold')\n",
    "plt.xlabel('Holiday Type')\n",
    "plt.ylabel('Average Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Additional** and **Transfer** holidays result in the highest average sales, suggesting these holidays might drive significant consumer activity.\n",
    "- **Bridge**, **Event**, and **Work Day** holidays also show relatively high average sales, indicating their potential importance for sales strategies.\n",
    "- **Non-Holidays** (None) have the lowest average sales, which could imply that regular days without any special events or holidays are less likely to drive high sales.\n",
    "- The high standard deviations across all categories indicate a wide variability in sales, highlighting the need for targeted marketing strategies to maximize sales on different holiday types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. What analysis can we get from the date and its extractable features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting date features\n",
    "train_merged['quarter'] = train_merged['date'].dt.quarter\n",
    "train_merged['is_weekend'] = train_merged['dayofweek'] >= 5\n",
    "test_merged['quarter'] = test_merged['date'].dt.quarter\n",
    "test_merged['is_weekend'] = test_merged['dayofweek'] >= 5\n",
    "\n",
    "# Calculating average sales by day of the week\n",
    "sales_by_day_of_week = train_merged.groupby('dayofweek')['sales'].mean()\n",
    "\n",
    "# Calculating average sales by month\n",
    "sales_by_month = train_merged.groupby('month')['sales'].mean()\n",
    "\n",
    "# Calculating average sales by quarter\n",
    "sales_by_quarter = train_merged.groupby('quarter')['sales'].mean()\n",
    "\n",
    "# Calculating average sales by year\n",
    "sales_by_year = train_merged.groupby('year')['sales'].mean()\n",
    "\n",
    "# Setting up the figure for subplots\n",
    "fig, axs = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "# Plotting average sales by day of the week\n",
    "axs[0, 0].bar(sales_by_day_of_week.index, sales_by_day_of_week, color=palette)\n",
    "axs[0, 0].set_title('Average Sales by Day of the Week',fontsize=16, weight='bold')\n",
    "axs[0, 0].set_xlabel('Day of the Week')\n",
    "axs[0, 0].set_ylabel('Average Sales')\n",
    "axs[0, 0].set_xticks(range(7))\n",
    "axs[0, 0].set_xticklabels(['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
    "\n",
    "# Plotting average sales by month\n",
    "axs[0, 1].bar(sales_by_month.index, sales_by_month, color=palette)\n",
    "axs[0, 1].set_title('Average Sales by Month', fontsize=16, weight='bold')\n",
    "axs[0, 1].set_xlabel('Month')\n",
    "axs[0, 1].set_ylabel('Average Sales')\n",
    "\n",
    "# Plotting average sales by quarter\n",
    "axs[1, 0].bar(sales_by_quarter.index, sales_by_quarter, color=palette)\n",
    "axs[1, 0].set_title('Average Sales by Quarter', fontsize=16, weight='bold')\n",
    "axs[1, 0].set_xlabel('Quarter')\n",
    "axs[1, 0].set_ylabel('Average Sales')\n",
    "\n",
    "# Plotting average sales by year\n",
    "axs[1, 1].bar(sales_by_year.index, sales_by_year, color=palette)\n",
    "axs[1, 1].set_title('Average Sales by Year', fontsize=16, weight='bold')\n",
    "axs[1, 1].set_xlabel('Year')\n",
    "axs[1, 1].set_ylabel('Average Sales')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average Sales by Day of the Week\n",
    "- **Saturday and Sunday** have the highest average sales, indicating increased consumer activity during weekends.\n",
    "- **Thursday** shows the lowest average sales among the days of the week, suggesting a mid-week dip in sales.\n",
    "- Sales are relatively steady from Monday to Wednesday and pick up significantly on Friday, leading into the weekend.\n",
    "\n",
    " Average Sales by Month\n",
    "- **December** has the highest average sales, likely due to holiday shopping and end-of-year activities.\n",
    "- **January** and **February** have relatively lower sales, which might be due to post-holiday effects and the start of the new year.\n",
    "- The middle months show consistent sales with slight variations, indicating steady consumer activity throughout the year.\n",
    "\n",
    " Average Sales by Quarter\n",
    "- **Q4** shows the highest average sales, driven by the holiday season and increased consumer spending in the final months of the year.\n",
    "- The other quarters (Q1, Q2, Q3) have relatively similar average sales, with no significant fluctuations, indicating stable sales performance across these periods.\n",
    "\n",
    " Average Sales by Year\n",
    "- There is a clear upward trend in average sales from **2013 to 2017**, indicating overall growth in sales performance over these years.\n",
    "- The most significant increase is observed from **2013 to 2014**, followed by steady growth in subsequent years, suggesting positive business growth and potentially successful marketing or expansion strategies.\n",
    "\n",
    "#### Key Takeaways\n",
    "- **Weekend Sales**: The highest sales on weekends highlight the importance of targeting weekend shoppers with promotions and marketing efforts.\n",
    "- **Seasonal Trends**: December's peak in sales underscores the significance of holiday seasons and the need for strategic planning during this period.\n",
    "- **Quarterly Analysis**: The strong performance in Q4 suggests focusing on the last quarter for maximizing sales opportunities.\n",
    "- **Yearly Growth**: The consistent growth year-over-year indicates a positive trajectory, with potential for further expansion and increased sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Which product family and stores did the promotions affect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering for promotions in the holidays_events data\n",
    "promotion_days = holidays_events[holidays_events['type']== 'Event']['date']\n",
    "\n",
    "# Adding a column in the train and test data to indicate if a day is a promotion day\n",
    "train_merged['is_promotion'] = train_merged['date'].isin(promotion_days)\n",
    "test_merged['is_promotion'] = test_merged['date'].isin(promotion_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating sales during promotion and non-promotion periods\n",
    "sales_during_promotion = train_merged[train_merged['is_promotion']]['sales'].mean()\n",
    "sales_non_promotion = train_merged[~train_merged['is_promotion']]['sales'].mean()\n",
    "\n",
    "print(f\"Average sales during promotions: {sales_during_promotion}\")\n",
    "print(f\"Average sales during non-promotion periods: {sales_non_promotion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating sales by product family\n",
    "product_family_sales = pd.DataFrame(train_merged.groupby(['family', 'is_promotion'])['sales'].mean().unstack())\n",
    "      \n",
    "# Calculating the difference between promotion and non-promotion periods\n",
    "product_family_sales['difference'] = product_family_sales[True] - product_family_sales[False]\n",
    "\n",
    "# Filtering top 5 product families based on sales difference\n",
    "top_5_product_families = product_family_sales.nlargest(5, 'difference')\n",
    "top_5_product_families"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate sales by store\n",
    "store_sales = pd.DataFrame(train_merged.groupby(['store_nbr', 'is_promotion'])['sales'].mean().unstack())\n",
    "\n",
    "# Calculating the difference between promotion and non-promotion periods\n",
    "store_sales['difference'] = store_sales[True] - store_sales[False]\n",
    "\n",
    "# Filtering top 5 product families based on sales difference\n",
    "top_5_product_families = store_sales.nlargest(5, 'difference')\n",
    "top_5_product_families"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. What is the difference between RMSLE, RMSE, MSE (or why is the MAE greater than all of them?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Mean Squared Error (MSE)\n",
    "- **Definition**: The Mean Squared Error is the average of the squared differences between the predicted and actual values.\n",
    "- **Properties**: \n",
    "  - It penalizes larger errors more than smaller ones due to the squaring of errors.\n",
    "  - Sensitive to outliers because squaring magnifies large errors.\n",
    "\n",
    "#### b. Root Mean Squared Error (RMSE)\n",
    "- **Definition**: The Root Mean Squared Error is the square root of the Mean Squared Error.\n",
    "- **Properties**:\n",
    "  - It has the same units as the target variable.\n",
    "  - Like MSE, it penalizes larger errors more due to squaring before averaging.\n",
    "\n",
    "#### c. Root Mean Squared Logarithmic Error (RMSLE)\n",
    "- **Definition**: The Root Mean Squared Logarithmic Error measures the ratio between the actual and predicted values by comparing the logarithms of the predicted and actual values.\n",
    "- **Properties**:\n",
    "  - Useful when the target variable has exponential growth or when the relative error is more important than the absolute error.\n",
    "  - Less sensitive to large differences for high target values.\n",
    "\n",
    "#### d. Mean Absolute Error (MAE)\n",
    "- **Definition**: The Mean Absolute Error is the average of the absolute differences between the predicted and actual values.\n",
    "- **Properties**:\n",
    "  - It has the same units as the target variable.\n",
    "  - It treats all errors equally, without squaring them.\n",
    "  - Less sensitive to outliers compared to MSE and RMSE.\n",
    "\n",
    "#### Why is MAE Often Greater than RMSLE, RMSE, and MSE?\n",
    "\n",
    "The MAE can be greater than RMSLE, RMSE, and MSE for the following reasons:\n",
    "\n",
    "a. **Penalization of Errors**:\n",
    "   - MSE and RMSE penalize larger errors more than smaller ones because of the squaring operation. This can lead to a lower overall average error when large errors are infrequent.\n",
    "   - RMSLE focuses on the logarithmic difference, which means it is less sensitive to large absolute errors but more sensitive to relative differences, particularly when the true values are small.\n",
    "\n",
    "b. **Error Aggregation**:\n",
    "   - MAE aggregates errors linearly. It does not give extra weight to larger errors.\n",
    "   - In contrast, RMSE and MSE give extra weight to larger errors due to the squaring of the errors, which can reduce the impact of smaller errors on the overall metric.\n",
    "\n",
    "c. **Error Scale**:\n",
    "   - RMSLE takes the logarithm of the predicted and actual values, which scales down the impact of larger values. Therefore, the logarithmic transformation tends to reduce the magnitude of the errors before squaring and averaging.\n",
    "\n",
    "- **`MAE`** is often higher because it considers the absolute error without additional penalization.\n",
    "- **`MSE`** and **`RMSE`** can be lower when large errors are infrequent because they square the errors, reducing the impact of smaller errors.\n",
    "- **`RMSLE`** can be much lower if the absolute differences are not large relative to the true values because it works with the logarithmic scale. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Does the payment of wages in the public sector on the 15th and last days of the month influence the store sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove]ing any rows with missing or infinite sales values\n",
    "train_merged =train_merged[np.isfinite(train_merged['sales'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merged['is_15th'] = train_merged['day'] == 15\n",
    "train_merged['is_last_day'] = train_merged['date'] == (train_merged['date'] + pd.offsets.MonthEnd(0))\n",
    "test_merged['is_15th'] = test_merged['day'] == 15\n",
    "test_merged['is_last_day'] = test_merged['date'] == (test_merged['date'] + pd.offsets.MonthEnd(0))\n",
    "\n",
    "# Creating a column to identify if it's either the 15th or the last day\n",
    "train_merged['is_payday'] = train_merged['is_15th'] | train_merged['is_last_day']\n",
    "test_merged['is_payday'] = test_merged['is_15th'] | train_merged['is_last_day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating average sales\n",
    "avg_sales_15th = train_merged[train_merged['is_15th']]['sales'].mean()\n",
    "avg_sales_last_day = train_merged[train_merged['is_last_day']]['sales'].mean()\n",
    "avg_sales_payday = train_merged[train_merged['is_payday']]['sales'].mean()\n",
    "avg_sales_other_days = train_merged[~train_merged['is_payday']]['sales'].mean()\n",
    "\n",
    "print(f\"Average sales on 15th: {avg_sales_15th}\")\n",
    "print(f\"Average sales on last day of the month: {avg_sales_last_day}\")\n",
    "print(f\"Average sales on paydays (15th and last day): {avg_sales_payday}\")\n",
    "print(f\"Average sales on other days: {avg_sales_other_days}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sales on paydays\n",
    "sales_payday = train_merged[train_merged['is_payday']]['sales']\n",
    "\n",
    "# Sales on other days\n",
    "sales_other_days = train_merged[~train_merged['is_payday']]['sales']\n",
    "\n",
    "# Conducting t-test\n",
    "t_stat, p_value = ttest_ind(sales_payday, sales_other_days)\n",
    "\n",
    "print(f\"T-statistic: {t_stat}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"We Reject the null hypothesis: Public sector wage payments on the 15th and the last days of the month influence store sales.\")\n",
    "else:\n",
    "    print(\"We Fail to reject the null hypothesis: Public sector wage payments on the 15th and the last days of the month do not influence store sales.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Modeling** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stationarity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating sales by date\n",
    "sales_by_date = train_merged.groupby('date')['sales'].sum()\n",
    "\n",
    "# Performing Augmented Dickey-Fuller test\n",
    "adf_result = adfuller(sales_by_date)\n",
    "\n",
    "# Extracting the p-value from the test\n",
    "adf_p_value = adf_result[1]\n",
    "\n",
    "adf_p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing rolling mean and variance\n",
    "rolling_mean = sales_by_date.rolling(window=12).mean()\n",
    "rolling_std = sales_by_date.rolling(window=12).std()\n",
    "\n",
    "# Plotting rolling statistics\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(sales_by_date, label='Original')\n",
    "plt.plot(rolling_mean, color='red', label='Rolling Mean')\n",
    "plt.plot(rolling_std, color='black', label='Rolling Std')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.title('Rolling Mean & Standard Deviation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformation and Differencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying log transformation\n",
    "sales_log = np.log(sales_by_date + 1)\n",
    "\n",
    "# Computing rolling mean and variance on log-transformed data\n",
    "rolling_mean_log = sales_log.rolling(window=12).mean()\n",
    "rolling_std_log = sales_log.rolling(window=12).std()\n",
    "\n",
    "# Plotting rolling statistics on log-transformed data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(sales_log, label='Log Transformed')\n",
    "plt.plot(rolling_mean_log, color='red', label='Rolling Mean')\n",
    "plt.plot(rolling_std_log, color='black', label='Rolling Std')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Log Sales')\n",
    "plt.title('Log Transformed Rolling Mean & Standard Deviation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying differencing\n",
    "sales_diff = sales_log.diff().dropna()\n",
    "\n",
    "# Performing ADF test on differenced data\n",
    "adf_result_diff = adfuller(sales_diff)\n",
    "adf_p_value_diff = adf_result_diff[1]\n",
    "\n",
    "adf_p_value_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute rolling mean and variance on differenced data\n",
    "rolling_mean_diff = sales_diff.rolling(window=12).mean()\n",
    "rolling_std_diff = sales_diff.rolling(window=12).std()\n",
    "\n",
    "# Plot rolling statistics on differenced data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(sales_diff, label='Differenced')\n",
    "plt.plot(rolling_mean_diff, color='red', label='Rolling Mean')\n",
    "plt.plot(rolling_std_diff, color='black', label='Rolling Std')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Differenced Log Sales')\n",
    "plt.title('Differenced Rolling Mean & Standard Deviation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating lag features\n",
    "for lag in range(1, 8):\n",
    "    train_merged[f'sales_lag_{lag}'] = train_merged.groupby(['store_nbr', 'family'])['sales'].shift(lag)\n",
    "\n",
    "# Creating rolling mean and standard deviation features\n",
    "train_merged['rolling_mean_7'] = train_merged.groupby(['store_nbr', 'family'])['sales'].transform(lambda x: x.rolling(window=7).mean())\n",
    "train_merged['rolling_mean_7'] = train_merged.groupby(['store_nbr', 'family'])['sales'].transform(lambda x: x.rolling(window=7).mean())\n",
    "test_merged['rolling_std_7'] = test_merged.groupby(['store_nbr', 'family'])['sales'].transform(lambda x: x.rolling(window=7).std())\n",
    "test_merged['rolling_std_7'] = test_merged.groupby(['store_nbr', 'family'])['sales'].transform(lambda x: x.rolling(window=7).std())\n",
    "\n",
    "# Dropping rows with NaN values due to lagging\n",
    "train_merged = train_merged.dropna()\n",
    "test_merged = test_merged.dropna()\n",
    "\n",
    "# Preparing feature set and target variable\n",
    "categorical_features = ['family', 'city', 'state', 'store_type', 'holiday_type', 'locale', 'locale_name', 'description', 'transferred']\n",
    "numerical_features = [\n",
    "    'store_nbr', 'onpromotion', 'transactions', 'cluster', 'dcoilwtico', \n",
    "    'year', 'month', 'day', 'dayofweek', 'quarter', 'is_weekend', \n",
    "    'is_promotion', 'is_15th', 'is_last_day', 'is_payday', \n",
    "    'rolling_mean_7', 'rolling_std_7'\n",
    "]\n",
    "\n",
    "# Define features combining categorical and numerical\n",
    "features = categorical_features + numerical_features\n",
    "\n",
    "target = 'sales'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting boolean values to strings in the 'transferred' column\n",
    "train_merged['transferred'] = train_merged['transferred'].astype(str)\n",
    "test_merged['transferred'] = test_merged['transferred'].astype(str)\n",
    "\n",
    "# Applying label encoding to the 'transferred' column\n",
    "le_transferred = LabelEncoder()\n",
    "train_merged['transferred'] = le_transferred.fit_transform(train_merged['transferred'])\n",
    "test_merged['transferred'] = le_transferred.fit_transform(test_merged['transferred'])\n",
    "\n",
    "# Prepare feature set and target variable again\n",
    "X = train_merged[features]\n",
    "y = train_merged[target]\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling with Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARIMA/SARIMA Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARIMAModel(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, seasonal=True, m=12):\n",
    "        self.seasonal = seasonal\n",
    "        self.m = m\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model = auto_arima(y, seasonal=self.seasonal, m=self.m)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(n_periods=len(X))\n",
    "\n",
    "\n",
    "arima_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('arima', ARIMAModel())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exponential Smoothing (ETS) Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ETSPipeline(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, trend='add', seasonal='add', seasonal_periods=12):\n",
    "        self.trend = trend\n",
    "        self.seasonal = seasonal\n",
    "        self.seasonal_periods = seasonal_periods\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model = ExponentialSmoothing(y, trend=self.trend, seasonal=self.seasonal, seasonal_periods=self.seasonal_periods).fit()\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.forecast(len(X))\n",
    "\n",
    "\n",
    "ets_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('ets', ETSPipeline())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest and XGBoost Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('rf', RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('xgb', XGBRegressor(objective='reg:squarederror',\n",
    "     n_estimators=100, learning_rate=0.1, max_depth=5))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, input_shape=input_shape))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "class LSTMPipeline(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, epochs=10, batch_size=32):\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "        self.model = create_lstm_model((X.shape[1], 1))\n",
    "        self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, verbose=0)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "        return self.model.predict(X).flatten()\n",
    "\n",
    "\n",
    "lstm_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('lstm', LSTMPipeline())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of pipelines\n",
    "pipelines = {\n",
    "    'ARIMA': arima_pipeline,\n",
    "    'ETS': ets_pipeline,\n",
    "    'RandomForest': rf_pipeline,\n",
    "    'XGBoost': xgb_pipeline,\n",
    "    'LSTM': lstm_pipeline\n",
    "}\n",
    "\n",
    "# Training and evaluating each pipeline\n",
    "results = {}\n",
    "for name, pipeline in pipelines.items():\n",
    "    print(f'Training {name} model...')\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    results[name] = rmse\n",
    "    print(f'{name} RMSE: {rmse}')\n",
    "\n",
    "# Display the results\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Hyperparameter Tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Predictions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Saving**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
